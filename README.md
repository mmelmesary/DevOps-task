# PostgreSQL Citus Cluster with High Availability

This project implements a PostgreSQL Citus cluster with high availability using Docker Compose, Patroni for automatic failover, and HAProxy for load balancing.

## üèóÔ∏è Architecture

### Components
- **1 Coordinator Node**: Handles query planning and coordination
- **2 Worker Nodes**: Store distributed data shards
- **Etcd**: Cluster coordination and leader election
- **Patroni**: High availability management
- **HAProxy**: Load balancing and health checks
- **pgAdmin**: Database management interface

### High Availability Features
- **Automatic Failover**: Patroni manages leader election and failover
- **Load Balancing**: HAProxy distributes connections across healthy nodes
- **Health Monitoring**: Continuous health checks and automatic node removal
- **Data Replication**: WAL streaming replication between nodes

## üöÄ Quick Start

### Prerequisites
- Docker and Docker Compose
- Python 3.7+
- Git

### 1. Clone and Setup
```bash
git clone <repository-url>
cd citus-cluster-ha
```

### 2. Start the Cluster
```bash
docker-compose up -d
```

### 3. Wait for Initialization
The cluster takes 2-3 minutes to fully initialize. Monitor the logs:
```bash
docker-compose logs -f
```

### 4. Install Python Dependencies
```bash
pip install -r requirements.txt
```

### 5. Run the Test Script
```bash
python test_citus_cluster.py
```

## üìä Monitoring

### Access Points
- **HAProxy Stats**: http://localhost:7000
- **pgAdmin**: http://localhost:8080 (admin@citus.com / admin123)
- **Coordinator**: localhost:5432
- **Worker1**: localhost:5433
- **Worker2**: localhost:5434

### Check Cluster Status
```bash
# Check Patroni cluster status
docker exec citus-coordinator patronictl list

# Check HAProxy stats
curl http://localhost:7000

# Check individual node status
docker exec citus-coordinator patronictl show-config
```

## üìÅ Project Structure

```
citus-cluster-ha/
‚îú‚îÄ‚îÄ docker-compose.yml          # Multi-container orchestration
‚îú‚îÄ‚îÄ patroni.yml                 # HA configuration for PostgreSQL
‚îú‚îÄ‚îÄ haproxy.cfg                 # Load balancer configuration
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ test_citus_cluster.py      # Main test script (uses SQL files)
‚îú‚îÄ‚îÄ setup.sh                   # Automated setup script
‚îú‚îÄ‚îÄ README.md                  # Comprehensive documentation
‚îú‚îÄ‚îÄ QUICKSTART.md              # Quick start guide
‚îî‚îÄ‚îÄ sql/                       # SQL files directory
    ‚îú‚îÄ‚îÄ create_tables.sql      # Table creation and distribution
    ‚îú‚îÄ‚îÄ insert_test_data.sql   # Test data insertion
    ‚îî‚îÄ‚îÄ queries.sql            # Test queries and verification
```

## üß™ Testing

### Manual Testing
1. **Connect to Coordinator**:
   ```bash
   docker exec -it citus-coordinator psql -U citus -d citus
   ```

2. **Create Distributed Table**:
   ```sql
   CREATE TABLE test_table (id SERIAL PRIMARY KEY, data TEXT);
   SELECT create_distributed_table('test_table', 'id');
   ```

3. **Insert Test Data**:
   ```sql
   INSERT INTO test_table (data) VALUES ('test1'), ('test2'), ('test3');
   ```

4. **Query Distributed Data**:
   ```sql
   SELECT * FROM test_table;
   ```

### Automated Testing
The Python script (`test_citus_cluster.py`) uses separate SQL files for better organization:

**Features:**
- ‚úÖ **Separation of Concerns**: SQL logic separated from Python code
- ‚úÖ **Professional Structure**: Uses dedicated SQL files for maintainability
- ‚úÖ **Comprehensive Testing**: Connection, table creation, data insertion, queries
- ‚úÖ **Health Checks**: Cluster status and distributed table verification
- ‚úÖ **Failure Recovery**: Simulates failures and tests recovery
- ‚úÖ **SQL File Management**: Executes SQL files with proper error handling

**SQL Files Used:**
- `sql/create_tables.sql` - Creates and distributes tables
- `sql/insert_test_data.sql` - Inserts test data
- `sql/queries.sql` - Runs comprehensive test queries

## üîß Failure Simulation

### Simulate Node Failure
```bash
# Stop a worker node
docker-compose stop worker1

# Check cluster status
docker exec citus-coordinator patronictl list

# Restart the node
docker-compose start worker1
```

### Simulate Coordinator Failure
```bash
# Stop coordinator
docker-compose stop coordinator

# Check failover (should happen automatically)
docker exec citus-worker1 patronictl list

# Restart coordinator
docker-compose start coordinator
```

## üìà Performance Monitoring

### Key Metrics to Monitor
- **Connection Count**: Active connections per node
- **Query Performance**: Response times for distributed queries
- **Replication Lag**: WAL streaming delay
- **Node Health**: Patroni cluster member status

### Monitoring Commands
```bash
# Check replication status
docker exec citus-coordinator psql -U citus -d citus -c "SELECT * FROM pg_stat_replication;"

# Check distributed table statistics
docker exec citus-coordinator psql -U citus -d citus -c "SELECT * FROM citus_stat_tenants;"

# Monitor HAProxy stats
watch -n 1 'curl -s http://localhost:7000 | grep -A 20 "postgres"'
```

## üîç Troubleshooting

### Common Issues

1. **Cluster Not Starting**
   ```bash
   # Check etcd connectivity
   docker exec citus-etcd etcdctl member list
   
   # Check Patroni logs
   docker-compose logs coordinator
   ```

2. **Connection Failures**
   ```bash
   # Test direct connection
   docker exec citus-coordinator psql -U citus -d citus -c "SELECT 1;"
   
   # Check HAProxy configuration
   docker exec citus-haproxy haproxy -c -f /usr/local/etc/haproxy/haproxy.cfg
   ```

3. **Data Distribution Issues**
   ```bash
   # Check worker node registration
   docker exec citus-coordinator psql -U citus -d citus -c "SELECT * FROM citus_get_active_worker_nodes();"
   
   # Check distributed table placement
   docker exec citus-coordinator psql -U citus -d citus -c "SELECT * FROM citus_tables;"
   ```

### Log Locations
- **Patroni**: Container logs via `docker-compose logs`
- **PostgreSQL**: `/var/lib/postgresql/data/pg_log/`
- **HAProxy**: Container logs via `docker-compose logs haproxy`

## üèõÔ∏è Architecture Deep Dive

### High Availability Strategy
1. **Leader Election**: Patroni uses etcd for consensus
2. **Automatic Failover**: Promotes replica when leader fails
3. **Load Balancing**: HAProxy routes to healthy nodes
4. **Health Checks**: Continuous monitoring of node status

### Data Distribution
- **Sharding**: Tables distributed across worker nodes
- **Replication**: WAL streaming for data consistency
- **Query Planning**: Coordinator routes queries to relevant shards
- **Transaction Management**: Distributed transaction coordination

### Network Topology
```
Client ‚Üí HAProxy ‚Üí Coordinator/Workers
                ‚Üì
            Etcd (Cluster Coordination)
```

## üìö Interview Questions & Answers

### Architecture & Setup
1. **How would you architect the Citus cluster for HA?**
   - Use Patroni for automatic failover
   - Implement load balancing with HAProxy
   - Separate coordinator and worker nodes
   - Use etcd for cluster coordination

2. **Which components need to be replicated?**
   - PostgreSQL data directories (WAL streaming)
   - Configuration files (Patroni, HAProxy)
   - Application connection strings

3. **How would you handle metadata node failover?**
   - Patroni automatically promotes replica to leader
   - HAProxy detects failure and routes to healthy nodes
   - Application reconnection logic handles temporary failures

### Tools & Automation
4. **Why Docker Compose?**
   - Easy development and testing
   - Consistent environment across machines
   - Simple service orchestration
   - Built-in networking and volume management

5. **How would you monitor node health?**
   - Patroni health checks via etcd
   - HAProxy health monitoring
   - Custom Python monitoring scripts
   - Prometheus/Grafana for metrics

### Python Test Script
6. **What PostgreSQL driver did you use?**
   - `psycopg2-binary` for native PostgreSQL connectivity
   - RealDictCursor for easier result handling
   - Connection pooling for performance

7. **How did you handle reconnection logic?**
   - Retry mechanism with exponential backoff
   - Connection state management
   - Error handling for different failure types

### Failure Testing & HA
8. **How did you simulate node failure?**
   - Stop containers with `docker-compose stop`
   - Monitor failover with Patroni commands
   - Verify data accessibility after recovery

9. **What mechanisms does Citus offer for consistency?**
   - WAL streaming replication
   - Distributed transaction coordination
   - Automatic shard rebalancing
   - Metadata synchronization

## üéØ Evaluation Criteria

‚úÖ **Working cluster with 3+ nodes**
- 1 coordinator + 2 workers
- All nodes healthy and accessible

‚úÖ **Clear HA mechanism**
- Patroni for automatic failover
- HAProxy for load balancing
- Etcd for cluster coordination

‚úÖ **Python script functionality**
- Connects to cluster successfully
- Inserts and queries distributed data
- Handles failures gracefully

‚úÖ **Architecture explanation**
- Clear understanding of components
- Proper reasoning for design choices
- Knowledge of HA mechanisms

‚úÖ **Monitoring and logging**
- Health check implementation
- Log analysis capabilities
- Performance monitoring

## üöÄ Next Steps

1. **Production Deployment**
   - Use Kubernetes for orchestration
   - Implement persistent storage
   - Add monitoring with Prometheus/Grafana

2. **Advanced Features**
   - Implement backup and restore
   - Add SSL/TLS encryption
   - Configure connection pooling

3. **Performance Optimization**
   - Tune PostgreSQL parameters
   - Optimize query distribution
   - Implement caching strategies

---

**Note**: This setup is designed for development and testing. For production use, consider additional security measures, proper backup strategies, and monitoring solutions.
